% !TEX root = ../notes_template.tex
\chapter{Matrices}\label{chp:vectors}

% \minitoc

\section{Matrcies}
Matrices are rectangular arrangement of numbers, with a finite number of rows and columns. Matrices are represented as rectangular grid of numbers with $n$ rows and $m$ columns, with the grid contained between two square brackets, as shown below:
%\vspace{0.15cm}
\begin{center}
\begin{tikzpicture}[scale=0.6]
\draw[thick, ->] (0,0) -- (0, -4.3) node[midway,left] {$n$ rows};
\draw[thick, ->] (0.5,0.5) -- (6.1, 0.5) node[midway,above]{$m$ columns};
%\node[xshift=-0.6cm,yshift=-1.3cm] {Rows};
\node[gray,xshift=2.0cm,yshift=-1.3cm] {$\begin{bmatrix}
\Box & \Box & \Box & \ldots & \Box \\
\Box & \Box & \Box & \ldots & \Box \\
\Box & \textcolor{black}{a_{32}} & \Box & \ldots & \Box \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\Box & \Box & \Box & \ldots & \Box \\
\end{bmatrix}$};
\draw[draw=red] (0.5,-2.35) rectangle ++(5.6,0.8);
\draw[draw=blue] (1.6,-4.25) rectangle ++(1.0,4.25);
\draw [thick, blue, ->] (2.15,-4.4) to [out=-90,in=180] (3.0, -5.0) node[right] {\small{$2^{nd}$ column}};
\draw [thick, red, ->] (6.2,-1.95) to (7.,-1.95) node[right] {\small{$3^{rd}$ row}};
\end{tikzpicture}\hspace{0.5cm}
\end{center}
The matrix is referred to as an $n \times m$ matrix, where $n$ is the number of rows and $m$ is the number of columns. The numbers in the matrix are called the \textit{elements} of the matrix. The element in the $i^{th}$ row and $j^{th}$ column of the matrix is denoted by $a_{ij}$. The elements of a matrix with the same row and column index are called the \textit{diagonal elements} of the matrix; these are elements of the form $a_{ii}$.

We will denote matrices using bold capital letters, and its elements by the corresponding lowercase letter with the row and column indices as subscripts; the row index will be written first, followed by the column index. For example, in the picture above $a_{32}$ is the element corresponding to the element in the $3^{rd}$ row and $2^{nd}$ column. This must make you wonder if the $n$-vectors we talked about earlier are just $n \times 1$ matrices. You are right! We can interpret these as $n \times 1$ matrices. In fact, these single column matrices are also referred to as \textit{column vectors} or a \textit{column matrix}. Now hold on, does that mean that we can also have \textit{row vectors} or \textit{row matrices}? Yes, we do have row vectors, which are $1 \times m$ matrices. We will talk about these in a later section in this chapter. Note that we can also have a $1 \times 1$ matrix!

Depending on the number of rows and columns, we group matrices in three categories based on their shape:
\begin{itemize}
    \item \textbf{Square matrix:} A matrix is said to be square if it has the same number of rows and columns, $n = m$. A square matrix is denoted by $n \times n$.
    \item \textbf{Wide/Fat matrix:} A matrix with more columns than rows, $n < m$.
    \item \textbf{Tall/Skinny matrix:} A matrix with more rows than columns, $n > m$.
\end{itemize}
We will refer to $n \times m$ as the \textit{shape of a matrix} $\mf{A}$, where $n, m$ are the number of rows and columns of $\mf{A}$, respectively. The set of all $n \times m$ matrices is denoted by the set $\mb{R}^{n \times m}$, where $\mb{R}$ is the set of real numbers. Later on we will come across matrices where the elements are complex numbers and these would be elements from the set $\mb{C}^{n \times m}$.

\noindent\textbf{Block Matrices and Submatrices}: We will often also come across matrices where the elements themselves are matrices. These are called \textit{block matrices}. The following is an example of a block matrix $\mf{M}$:
\begin{equation}
    \mf{M} = \bmxc \mf{A}_1 & \mf{A}_2 \\ \mf{A}_3 & \mf{A}_4 \emx
    \label{eq:ch02-block-mat}
\end{equation}
Here, $\mf{A}_1, \mf{A}_2, \mf{A}_3, \mf{A}_4$ are themselves matrices, which are refered to as the submatrices of $\mf{M}$. We cannot have any arbitrary matrices as submatrices of a block matrix. The submatrices must satisfy some constraints.
\begin{itemize}
    \item The submatrices in a column must have the same number of columns, but have arbitrary number of rows.
    \item The submatrices in a row must have the same number of rows, but have arbitrary number of columns.
\end{itemize}
Let the shape of the submatrix $\mf{A}_i$ in Eq.~\ref{eq:ch02-block-mat} be $n_i \times m_i$. Then, $n_1 = n_2$, $n_3 = n_4$, $m_1 = m_3$, and $m_2 = m_4$. The shape of the block matrix $\mf{M}$ is $\pp{n_1 + n_3} \times \pp{m_1 + m_2}$.

Matrices also are a conveninent way of represent a set of indexed column $n$-vectors, $\mf{x}_1, \mf{x}_2, \ldots \mf{x}_m$. We can treat these columns vectors are $n \times 1$ matrices and form a block matrix as shown below:
\begin{equation}
    \mf{X} = \bmxc \mf{x}_1 & \mf{x}_2 & \ldots & \mf{x}_m \emx
    \label{eq:ch02-block-col-vec}
\end{equation}
\textcolor{red}{Can I call this matrix a block row matrix? What is the shape of this matrix?}
\subsection{Some special matrices}
We will now define some special matrices that we will come across in this course.
\begin{itemize}
    \item \textbf{Zero matrix:} The matrix whose elements are all zeros is called the \textit{zero matrix}. These are often represented by $\mf{0}_{n \times m}$ --  the matrix of shape $n \times m$ with all elements as zeros.
    \item \textbf{Identity matrix:} The square matrix whose diagonal elements are all ones and all other elements are zeros is called the \textit{identity matrix}. The identity matrix of shape $n \times n$ is denoted by $\mf{I}_n$.
    \[ \mf{I}_2 = \bmxc 1 & 0 \\ 0 & 1 \emx \quad \mf{I}_3 = \bmxc 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \emx \]
    Notice that we can also represent identify matrices as a block row matrix of the following form,
    \[ \mf{I}_n = \bmxc \mf{e}_1 & \mf{e}_2 & \ldots & \mf{e}_n \emx \]
    where, $\mf{e}_1, \mf{e}_2, \ldots \mf{e}_n$ are the $n$-unit vectors of $\mb{R}^n$.
    \item \textbf{Diagonal matrix:} A square matrix whose non-diagonal elements are all zeros is called a \textit{diagonal matrix}. The diagonal matrix of shape $n \times n$ with diagonal elements $d_1, d_2, \ldots d_n$ is denoted by $\mf{D}$.
    \[ \bmxc d_1 & 0 & \ldots & 0 \\ 0 & d_2 & \ldots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \ldots & d_n \emx = \textbf{diag}\pp{d_1, d_2, \dots d_n}\]
    \item \textbf{Upper triangular matrix:} A square matrix whose elements below the diagonal are all zeros is called an \textit{upper triangular matrix}.
    \item \textbf{Lower triangular matrix:} A square matrix whose elements above the diagonal are all zeros is called a \textit{lower triangular matrix}.
    \item \textbf{Sparse matrix:} A matrix with a large number of zeros is called a \textit{sparse matrix}. We will not come across sparse matrices in this course.
\end{itemize}

\subsection{Why do I need to know about matrices?}
Matrices are a fundamental concept in linear algebra, and are used in many applications. There are two ways to interpret a matrix -- the rectangular arrangment of numbers:

\vspace{0.2cm}
\noindent \textbf{Data representation:} Matrices are a convenient way to represent data. Any application where there is a set of parameters are measured multiple times - across space, time, individuals, etc. These are usually represented as a table of entires. A table of entries can be thought of as a matrix. For example, 
\begin{itemize}
    \item The temperature recorded at different locations at different times can be represented as a matrix; the different locations could correspond to the columns and different measurement timepoints could correspond rows.
    \item The clinical information of patients visiting a hospital can be represented as a matrix. The different patient details could the columns and the rows could correspond to different patients.
    \item A grayscale image is a matrix, with each element representing the the intensity of a pixel located at a partiular horizontal and vertical position.
    \item and so on $\ldots$
\end{itemize}

\vspace{0.2cm}
\noindent \textbf{Linear transformations:} Matrices are used to represent linear transformations. A linear transformation is a function that maps a vector to another vector, which means these can be use to manipulate vectors. We will have a detailed discussion on this section \hl{xxx}.

\section{Matrix operations}
We will now define some important matrix operations involving matrices. These are the operations that we will use in this course.

\subsection{Matrix transpose}
This may sound like a strange operation at first, but it turns out be an important operation. The transpose of a matrix is obtained by interchanging the rows and columns of the matrix. The transpose of a matrix $\mf{A} \in \mb{R}^{n \times m}$ is denoted by $\mf{A}^\top$. This matrix is a member of the set $\mb{R}^{m \times n}$. The element in the $i^{th}$ row and $j^{th}$ column of the matrix $\mf{A}$ is the $j^{th}$ row and $i^{th}$ column of the matrix $\mf{A}^\top$. The transpose of a matrix is defined as:
\[ \mf{A} = \bmxc a_{11} & a_{12} & \ldots & a_{1m} \\ a_{21} & a_{22} & \ldots & a_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \ldots & a_{nm} \emx \longrightarrow \mf{A}^\top = \bmxc a_{11} & a_{21} & \ldots & a_{n1} \\ a_{12} & a_{22} & \ldots & a_{n2} \\ \vdots & \vdots & \ddots & \vdots \\ a_{1m} & a_{2m} & \ldots & a_{nm} \emx \]

\begin{boxedstuff}
    \begin{problem}
        What is the transpose of the block matrix $\mf{M} = \bmxc \mf{A}_1 & \mf{A}_2 \\ \mf{A}_3 & \mf{A}_4 \emx$?.
    \end{problem}
\end{boxedstuff}

\subsection{Matrix scalar multiplication}
We can also multiply a matrix by a scalar. Given a scalar $c \in \mb{R}$ and a matrix $\mf{A} \in \mb{R}^{n \times m}$, the scalar multiplication operation produces another matrix $c\mf{A}$ whose elements are $ca_{11}, ca_{12}, \ldots, ca_{nm}$. The scalar multiplication operation is defined as:
\begin{equation}
    c\mf{A} = c\bmxc a_{11} & a_{12} & \ldots & a_{1m} \\ a_{21} & a_{22} & \ldots & a_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \ldots & a_{nm} \emx = \bmxc ca_{11} & ca_{12} & \ldots & ca_{1m} \\ ca_{21} & ca_{22} & \ldots & ca_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ ca_{n1} & ca_{n2} & \ldots & ca_{nm} \emx \in \mb{R}^{n \times m}
    \label{eq:ch02-mat-scalar-mult}
\end{equation}

\subsection{Matrix addition}
We can define a matrix addition operation between two matrices. We can add two matrices only if they have the same shape, i.e. they have the same number of rows and same number of columns. Consider two matrices $\mf{A}, \mf{B} \in \mb{R}^{n \times m}$. The addition of these matrices is defined as follows:
\begin{equation}
    \begin{split}
    \mf{A} + \mf{B} &= \bmxc a_{11} & a_{12} & \ldots & a_{1m} \\ a_{21} & a_{22} & \ldots & a_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \ldots & a_{nm} \emx + \bmxc b_{11} & b_{12} & \ldots & b_{1m} \\ b_{21} & b_{22} & \ldots & b_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ b_{n1} & b_{n2} & \ldots & b_{nm} \emx \\    
    &= \bmxc a_{11} + b_{11} & a_{12} + b_{12} & \ldots & a_{1m} + b_{1m} \\ a_{21} + b_{21} & a_{22} + b_{22} & \ldots & a_{2m} + b_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} + b_{n1} & a_{n2} + b_{n2} & \ldots & a_{nm} + b_{nm} \emx \in \mb{R}^{n \times m}
    \end{split}
    \label{eq:ch02-mat-add}
\end{equation}    
Its very simple. You simply add the individual elements of the two matrices to get the elements of the resulting matrix. The resulting matrix is also a member of the set $\mb{R}^{n \times m}$. Note that this is consistent with the definition of the vector addition operation defined in the previous chapter.

\subsubsection{Properties of matrix addition}
Here are some of the properties of matrix addition:
\begin{itemize}
    \item \textbf{Commutative property:} $\mf{A} + \mf{B} = \mf{B} + \mf{A}$.
    \item \textbf{Associative property:} $\mf{A} + \pp{\mf{B} + \mf{C}} = \pp{\mf{A} + \mf{B}} + \mf{C}$.
    \item \textbf{Distributive property:} $c\pp{\mf{A} + \mf{B}} = c\mf{A} + c\mf{B}$.
    \item \textbf{Additiob with the zero matrix:} $\mf{A} + \mf{0}_{n \times m} = \mf{A}$.
    \item \textbf{Tranpose of the sum:} $\pp{\mf{A} + \mf{B}}^\top = \mf{A}^\top + \mf{B}^\top$.
\end{itemize}    
We leave it as a exercise for you to prove these properties using the properties of real number addition.

\begin{boxedstuff}
    \begin{problem}
        \textbf{The set of matrices $\mf{R}^{n\times m}$ for a vector space!} The set of $n \times m$ matrices with real numbers is the set $\mb{R}^{n \times m}$. We have defined scalar matrix multiplication and matrix addition operations on the elements of this set. Show that the set $\mb{R}^{n \times m}$ is a vector space. 
        
        \noindent\begin{small}\textcolor{gray}{\textit{Hint}: You just need to show the set of closed under scalar multiplication and vector addition. You can also verify that the additional properties are also satisfied by this set.}\end{small}
    \end{problem}
\end{boxedstuff}

\subsection{Matrix multiplication}
This is the most important operation involving matrices. Understanding matrix multiplication is vital to understanding the rest of the course material. So the importance of this section cannot be overstated. Unlike matrix scalar multiplication and matrix addition, the concept of matrix multiplication will seem a bit strange at first. But we will see later that the definition of matrix multiplication is a natural when matrices are viewed as reprensenting linear transformations. 

Consder two matrices $\mf{A} \in \mb{R}^{n \times m}$ and $\mf{B} \in \mb{R}^{p \times q}$. A matric multiplication operation $\mf{A}\mf{B}$ is defined if and only if the number of columns of $\mf{A}$ is equal to the number of rows of $\mf{B}$, i.e. $m = p$. The result of the matrix multiplication operation is a matrix $\mf{C} \in \mb{R}^{n \times q}$, where $\mf{C} = \mf{A}\mf{B}$. The elements of the resulting matrix $\mf{C}$ are defined as:
\begin{equation}
    c_{ij} = \sum_{k=1}^{m} a_{ik}b_{kj}, \,\, \forall i \in \lc 1, \ldots n\rc, \,\, j \in \lc 1, \ldots q\rc
    \label{eq:ch02-mat-mult-ip}
\end{equation}
This for sure looks confusing and makes little sense. It turns out the matrix multiplication operation is a natural operation representing composition of linear transformation, and all matrices represent linear transformations. 

\begin{boxedstuff}
    \vspace{4mm}
    \noindent{\large \textbf{Hadamard product: Element-wise matrix multiplication} }
    \hrule
    \vspace{2mm}

    On a separate note, you are probably asking yourself, why not just define matrix multiplication like we defined matrix addition. Just multiply the individual elements together. the element-wise multilication is also a useful operation and is supported by scientific computing programs like MATLAB, Python, etc. This operation is called the \textit{Hadamard product} and is denoted by $\circ$. The Hadamard product of two matrices $\mf{A}, \mf{B} \in \mb{R}^{n \times m}$ is defined as:
    \begin{equation}
        \mf{A} \circ \mf{B} = \bmxc a_{11} & a_{12} & \ldots & a_{1m} \\ a_{21} & a_{22} & \ldots & a_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \ldots & a_{nm} \emx \circ \bmxc b_{11} & b_{12} & \ldots & b_{1m} \\ b_{21} & b_{22} & \ldots & b_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ b_{n1} & b_{n2} & \ldots & b_{nm} \emx = \bmxc a_{11}b_{11} & a_{12}b_{12} & \ldots & a_{1m}b_{1m} \\ a_{21}b_{21} & a_{22}b_{22} & \ldots & a_{2m}b_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1}b_{n1} & a_{n2}b_{n2} & \ldots & a_{nm}b_{nm} \emx \in \mb{R}^{n \times m}
        \label{eq:ch02-mat-hadamard}
    \end{equation}
    This element-wide multiplication operation is useful when matrices are used to represent data. For example, we might use the Hadamard product when wish to apply a mask to a image represented by the matrix. 
    
    We will not discuss the Hadamard product in this course, but it is good to know that this operation exists.
    We will not discuss the Hadamard product in this course, but it is good to know that this operation exists.
\end{boxedstuff}