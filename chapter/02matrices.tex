% !TEX root = ../notes_template.tex
\chapter{Matrices}\label{chp:vectors}

% \minitoc

\section{Matrcies}
Matrices are rectangular arrangement of numbers, with a finite number of rows and columns. Matrices are represented as rectangular grid of numbers with $n$ rows and $m$ columns, with the grid contained between two square brackets, as shown below:
%\vspace{0.15cm}
\begin{center}
\begin{tikzpicture}[scale=0.6]
\draw[thick, ->] (0,0) -- (0, -4.3) node[midway,left] {$n$ rows};
\draw[thick, ->] (0.5,0.5) -- (6.1, 0.5) node[midway,above]{$m$ columns};
%\node[xshift=-0.6cm,yshift=-1.3cm] {Rows};
\node[gray,xshift=2.0cm,yshift=-1.3cm] {$\begin{bmatrix}
\Box & \Box & \Box & \ldots & \Box \\
\Box & \Box & \Box & \ldots & \Box \\
\Box & \textcolor{black}{a_{32}} & \Box & \ldots & \Box \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\Box & \Box & \Box & \ldots & \Box \\
\end{bmatrix}$};
\draw[draw=red] (0.5,-2.35) rectangle ++(5.6,0.8);
\draw[draw=blue] (1.6,-4.25) rectangle ++(1.0,4.25);
\draw [thick, blue, ->] (2.15,-4.4) to [out=-90,in=180] (3.0, -5.0) node[right] {\small{$2^{nd}$ column}};
\draw [thick, red, ->] (6.2,-1.95) to (7.,-1.95) node[right] {\small{$3^{rd}$ row}};
\end{tikzpicture}\hspace{0.5cm}
\end{center}
The matrix is referred to as an $n \times m$ matrix, where $n$ is the number of rows and $m$ is the number of columns. The numbers in the matrix are called the \textit{elements} of the matrix. The element in the $i^{th}$ row and $j^{th}$ column of the matrix is denoted by $a_{ij}$. The elements of a matrix with the same row and column index are called the \textit{diagonal elements} of the matrix; these are elements of the form $a_{ii}$.

We will denote matrices using bold capital letters, and its elements by the corresponding lowercase letter with the row and column indices as subscripts; the row index will be written first, followed by the column index. For example, in the picture above $a_{32}$ is the element corresponding to the element in the $3^{rd}$ row and $2^{nd}$ column. This must make you wonder if the $n$-vectors we talked about earlier are just $n \times 1$ matrices. You are right! We can interpret these as $n \times 1$ matrices. In fact, these single column matrices are also referred to as \textit{column vectors} or a \textit{column matrix}. Now hold on, does that mean that we can also have \textit{row vectors} or \textit{row matrices}? Yes, we do have row vectors, which are $1 \times m$ matrices. We will talk about these in a later section in this chapter. Note that we can also have a $1 \times 1$ matrix!

Depending on the number of rows and columns, we group matrices in three categories based on their shape:
\begin{itemize}
    \item \textbf{Square matrix:} A matrix is said to be square if it has the same number of rows and columns, $n = m$. A square matrix is denoted by $n \times n$.
    \item \textbf{Wide/Fat matrix:} A matrix with more columns than rows, $n < m$.
    \item \textbf{Tall/Skinny matrix:} A matrix with more rows than columns, $n > m$.
\end{itemize}
We will refer to $n \times m$ as the \textit{shape of a matrix} $\mf{A}$, where $n, m$ are the number of rows and columns of $\mf{A}$, respectively. The set of all $n \times m$ matrices is denoted by the set $\mb{R}^{n \times m}$, where $\mb{R}$ is the set of real numbers. Later on we will come across matrices where the elements are complex numbers and these would be elements from the set $\mb{C}^{n \times m}$.

\noindent\textbf{Block Matrices and Submatrices}: We will often also come across matrices where the elements themselves are matrices. These are called \textit{block matrices}. The following is an example of a block matrix $\mf{M}$:
\begin{equation}
    \mf{M} = \bmxc \mf{A}_1 & \mf{A}_2 \\ \mf{A}_3 & \mf{A}_4 \emx
    \label{eq:ch02-block-mat}
\end{equation}
Here, $\mf{A}_1, \mf{A}_2, \mf{A}_3, \mf{A}_4$ are themselves matrices, which are refered to as the submatrices of $\mf{M}$. We cannot have any arbitrary matrices as submatrices of a block matrix. The submatrices must satisfy some constraints.
\begin{itemize}
    \item The submatrices in a column must have the same number of columns, but have arbitrary number of rows.
    \item The submatrices in a row must have the same number of rows, but have arbitrary number of columns.
\end{itemize}
Let the shape of the submatrix $\mf{A}_i$ in Eq.~\ref{eq:ch02-block-mat} be $n_i \times m_i$. Then, $n_1 = n_2$, $n_3 = n_4$, $m_1 = m_3$, and $m_2 = m_4$. The shape of the block matrix $\mf{M}$ is $\pp{n_1 + n_3} \times \pp{m_1 + m_2}$.

Matrices also are a conveninent way of represent a set of indexed column $n$-vectors, $\mf{x}_1, \mf{x}_2, \ldots \mf{x}_m$. We can treat these columns vectors are $n \times 1$ matrices and form a block matrix as shown below:
\begin{equation}
    \mf{X} = \bmxc \mf{x}_1 & \mf{x}_2 & \ldots & \mf{x}_m \emx
    \label{eq:ch02-block-col-vec}
\end{equation}
\textcolor{red}{Can I call this matrix a block row matrix? What is the shape of this matrix?}
\subsection{Some special matrices}
We will now define some special matrices that we will come across in this course.
\begin{itemize}
    \item \textbf{Zero matrix:} The matrix whose elements are all zeros is called the \textit{zero matrix}. These are often represented by $\mf{0}_{n \times m}$ --  the matrix of shape $n \times m$ with all elements as zeros.
    \item \textbf{Identity matrix:} The square matrix whose diagonal elements are all ones and all other elements are zeros is called the \textit{identity matrix}. The identity matrix of shape $n \times n$ is denoted by $\mf{I}_n$.
    \[ \mf{I}_2 = \bmxc 1 & 0 \\ 0 & 1 \emx \quad \mf{I}_3 = \bmxc 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \emx \]
    Notice that we can also represent identify matrices as a block row matrix of the following form,
    \[ \mf{I}_n = \bmxc \mf{e}_1 & \mf{e}_2 & \ldots & \mf{e}_n \emx \]
    where, $\mf{e}_1, \mf{e}_2, \ldots \mf{e}_n$ are the $n$-unit vectors of $\mb{R}^n$.
    \item \textbf{Diagonal matrix:} A square matrix whose non-diagonal elements are all zeros is called a \textit{diagonal matrix}. The diagonal matrix of shape $n \times n$ with diagonal elements $d_1, d_2, \ldots d_n$ is denoted by $\mf{D}$.
    \[ \bmxc d_1 & 0 & \ldots & 0 \\ 0 & d_2 & \ldots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \ldots & d_n \emx = \textbf{diag}\pp{d_1, d_2, \dots d_n}\]
    \item \textbf{Upper triangular matrix:} A square matrix whose elements below the diagonal are all zeros is called an \textit{upper triangular matrix}.
    \item \textbf{Lower triangular matrix:} A square matrix whose elements above the diagonal are all zeros is called a \textit{lower triangular matrix}.
    \item \textbf{Sparse matrix:} A matrix with a large number of zeros is called a \textit{sparse matrix}. We will not come across sparse matrices in this course.
\end{itemize}

\subsection{Why do I need to know about matrices?}
Matrices are a fundamental concept in linear algebra, and are used in many applications. There are two ways to interpret a matrix -- the rectangular arrangment of numbers:

\vspace{0.2cm}
\noindent \textbf{Data representation:} Matrices are a convenient way to represent data. Any application where there is a set of parameters are measured multiple times - across space, time, individuals, etc. These are usually represented as a table of entires. A table of entries can be thought of as a matrix. For example, 
\begin{itemize}
    \item The temperature recorded at different locations at different times can be represented as a matrix; the different locations could correspond to the columns and different measurement timepoints could correspond rows.
    \item The clinical information of patients visiting a hospital can be represented as a matrix. The different patient details could the columns and the rows could correspond to different patients.
    \item A grayscale image is a matrix, with each element representing the the intensity of a pixel located at a partiular horizontal and vertical position.
    \item and so on $\ldots$
\end{itemize}

\vspace{0.2cm}
\noindent \textbf{Linear transformations:} Matrices are used to represent linear transformations. A linear transformation is a function that maps a vector to another vector, which means these can be use to manipulate vectors. We will have a detailed discussion on this section \hl{xxx}.

\section{Matrix operations}
We will now define some important matrix operations involving matrices. These are the operations that we will use in this course.

\subsection{Matrix transpose}
This may sound like a strange operation at first, but it turns out be an important operation. The transpose of a matrix is obtained by interchanging the rows and columns of the matrix. The transpose of a matrix $\mf{A} \in \mb{R}^{n \times m}$ is denoted by $\mf{A}^\top$. This matrix is a member of the set $\mb{R}^{m \times n}$. The element in the $i^{th}$ row and $j^{th}$ column of the matrix $\mf{A}$ is the $j^{th}$ row and $i^{th}$ column of the matrix $\mf{A}^\top$. The transpose of a matrix is defined as:
\[ \mf{A} = \bmxc a_{11} & a_{12} & \ldots & a_{1m} \\ a_{21} & a_{22} & \ldots & a_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \ldots & a_{nm} \emx \longrightarrow \mf{A}^\top = \bmxc a_{11} & a_{21} & \ldots & a_{n1} \\ a_{12} & a_{22} & \ldots & a_{n2} \\ \vdots & \vdots & \ddots & \vdots \\ a_{1m} & a_{2m} & \ldots & a_{nm} \emx \]

\begin{boxedstuff}
    \begin{problem}
        What is the transpose of the block matrix $\mf{M} = \bmxc \mf{A}_1 & \mf{A}_2 \\ \mf{A}_3 & \mf{A}_4 \emx$?.
    \end{problem}
\end{boxedstuff}

\subsection{Matrix scalar multiplication}
We can also multiply a matrix by a scalar. Given a scalar $c \in \mb{R}$ and a matrix $\mf{A} \in \mb{R}^{n \times m}$, the scalar multiplication operation produces another matrix $c\mf{A}$ whose elements are $ca_{11}, ca_{12}, \ldots, ca_{nm}$. The scalar multiplication operation is defined as:
\begin{equation}
    c\mf{A} = c\bmxc a_{11} & a_{12} & \ldots & a_{1m} \\ a_{21} & a_{22} & \ldots & a_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \ldots & a_{nm} \emx = \bmxc ca_{11} & ca_{12} & \ldots & ca_{1m} \\ ca_{21} & ca_{22} & \ldots & ca_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ ca_{n1} & ca_{n2} & \ldots & ca_{nm} \emx \in \mb{R}^{n \times m}
    \label{eq:ch02-mat-scalar-mult}
\end{equation}

\subsection{Matrix addition}
We can define a matrix addition operation between two matrices. We can add two matrices only if they have the same shape, i.e. they have the same number of rows and same number of columns. Consider two matrices $\mf{A}, \mf{B} \in \mb{R}^{n \times m}$. The addition of these matrices is defined as follows:
\begin{equation}
    \begin{split}
    \mf{A} + \mf{B} &= \bmxc a_{11} & a_{12} & \ldots & a_{1m} \\ a_{21} & a_{22} & \ldots & a_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \ldots & a_{nm} \emx + \bmxc b_{11} & b_{12} & \ldots & b_{1m} \\ b_{21} & b_{22} & \ldots & b_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ b_{n1} & b_{n2} & \ldots & b_{nm} \emx \\    
    &= \bmxc a_{11} + b_{11} & a_{12} + b_{12} & \ldots & a_{1m} + b_{1m} \\ a_{21} + b_{21} & a_{22} + b_{22} & \ldots & a_{2m} + b_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} + b_{n1} & a_{n2} + b_{n2} & \ldots & a_{nm} + b_{nm} \emx \in \mb{R}^{n \times m}
    \end{split}
    \label{eq:ch02-mat-add}
\end{equation}    
Its very simple. You simply add the individual elements of the two matrices to get the elements of the resulting matrix. The resulting matrix is also a member of the set $\mb{R}^{n \times m}$. Note that this is consistent with the definition of the vector addition operation defined in the previous chapter.

\subsubsection{Properties of matrix addition}
Here are some of the properties of matrix addition:
\begin{itemize}
    \item \textbf{Commutative property:} $\mf{A} + \mf{B} = \mf{B} + \mf{A}$.
    \item \textbf{Associative property:} $\mf{A} + \pp{\mf{B} + \mf{C}} = \pp{\mf{A} + \mf{B}} + \mf{C}$.
    \item \textbf{Distributive property:} $c\pp{\mf{A} + \mf{B}} = c\mf{A} + c\mf{B}$.
    \item \textbf{Additiob with the zero matrix:} $\mf{A} + \mf{0}_{n \times m} = \mf{A}$.
    \item \textbf{Tranpose of the sum:} $\pp{\mf{A} + \mf{B}}^\top = \mf{A}^\top + \mf{B}^\top$.
\end{itemize}    
We leave it as a exercise for you to prove these properties using the properties of real number addition.

\begin{boxedstuff}
    \begin{problem}
        \textbf{The set of matrices $\mf{R}^{n\times m}$ for a vector space!} The set of $n \times m$ matrices with real numbers is the set $\mb{R}^{n \times m}$. We have defined scalar matrix multiplication and matrix addition operations on the elements of this set. Show that the set $\mb{R}^{n \times m}$ is a vector space. 
        
        \noindent\begin{small}\textcolor{gray}{\textit{Hint}: You just need to show the set of closed under scalar multiplication and vector addition. You can also verify that the additional properties are also satisfied by this set.}\end{small}
    \end{problem}
\end{boxedstuff}

\subsection{Matrix multiplication}
This is the most important operation involving matrices. Understanding matrix multiplication is vital to understanding the rest of the course material. So the importance of this section cannot be overstated. Unlike matrix scalar multiplication and matrix addition, the concept of matrix multiplication will seem a bit strange at first. But we will see later that the definition of matrix multiplication is a natural when matrices are viewed as reprensenting linear transformations. 

Consder two matrices $\mf{A} \in \mb{R}^{n \times m}$ and $\mf{B} \in \mb{R}^{p \times q}$. A matric multiplication operation $\mf{A}\mf{B}$ is defined if and only if the number of columns of $\mf{A}$ is equal to the number of rows of $\mf{B}$, i.e. $m = p$. The result of the matrix multiplication operation is a matrix $\mf{C} \in \mb{R}^{n \times q}$, where $\mf{C} = \mf{A}\mf{B}$. The elements of the resulting matrix $\mf{C}$ are defined as:
\begin{equation}
    c_{ij} = \sum_{k=1}^{m} a_{ik}b_{kj}, \,\, \forall i \in \lc 1, \ldots n\rc, \,\, j \in \lc 1, \ldots q\rc
    \label{eq:ch02-mat-mult}
\end{equation}
This for sure looks confusing and makes little sense. It turns out the matrix multiplication operation is a natural operation representing composition of linear transformation, and all matrices represent linear transformations. 

\begin{boxedstuff}
    \vspace{4mm}
    \noindent{\large \textbf{Hadamard product: Element-wise matrix multiplication} }
    \hrule
    \vspace{2mm}

    On a separate note, you are probably asking yourself, why not just define matrix multiplication like we defined matrix addition. Just multiply the individual elements together. the element-wise multilication is also a useful operation and is supported by scientific computing programs like MATLAB, Python, etc. This operation is called the \textit{Hadamard product} and is denoted by $\circ$. The Hadamard product of two matrices $\mf{A}, \mf{B} \in \mb{R}^{n \times m}$ is defined as:
    \begin{equation}
        \mf{A} \circ \mf{B} = \bmxc a_{11} & a_{12} & \ldots & a_{1m} \\ a_{21} & a_{22} & \ldots & a_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \ldots & a_{nm} \emx \circ \bmxc b_{11} & b_{12} & \ldots & b_{1m} \\ b_{21} & b_{22} & \ldots & b_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ b_{n1} & b_{n2} & \ldots & b_{nm} \emx = \bmxc a_{11}b_{11} & a_{12}b_{12} & \ldots & a_{1m}b_{1m} \\ a_{21}b_{21} & a_{22}b_{22} & \ldots & a_{2m}b_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1}b_{n1} & a_{n2}b_{n2} & \ldots & a_{nm}b_{nm} \emx \in \mb{R}^{n \times m}
        \label{eq:ch02-mat-hadamard}
    \end{equation}
    This element-wide multiplication operation is useful when matrices are used to represent data. For example, we might use the Hadamard product when wish to apply a mask to a image represented by the matrix. 
    
    We will not discuss the Hadamard product in this course, but it is good to know that this operation exists.
    We will not discuss the Hadamard product in this course, but it is good to know that this operation exists.
\end{boxedstuff}

The definition of matrix multiplication in Eq.~\ref{eq:ch02-mat-mult-ip} is hardly illuminating. Its hard to see what is going on. Before we dive a littel deeper into matrix multiplication, we will first define some coventions we will follow in the course:
\begin{itemize}
    \item All $n$-vectors are will be column vectors. Row vectors will be represented as the transpose of a column vector.
    \item For a matrix $\mf{A} \in \mb{R}^{n \times m}$, the columns of the matrix will be represented as $\mf{a}_1, \mf{a}_2, \ldots \mf{a}_m$, where each vector is from $\mb{R}^n$. Then, we can write the matrix as the following,
    \begin{equation}
        \mf{A} = \bmxc \mf{a}_1 & \mf{a}_2 & \ldots & \mf{a}_m \emx
        \label{eq:ch02-mat-col-rep}
    \end{equation}
    We represent the rows of the matrix $\mf{A}$ as $\tilde{\mf{a}}^\top_1, \tilde{\mf{a}}^\top_2, \ldots \tilde{\mf{a}}^\top_n$, where each vector $\tilde{\mf{a}}_i$ is a column vector from $\mb{R}^m$. We will reserve the tilde notation for the rows of a matrix. Then, we can write the matrix as the following,
    \begin{equation}
        \mf{A} = \bmxc \tilde{\mf{a}}_1 & \tilde{\mf{a}}_2 & \ldots & \tilde{\mf{a}}_n \emx^\top = \bmxc \tilde{\mf{a}}_1^\top \\ \tilde{\mf{a}}_2^\top \\ \vdots \\ \tilde{\mf{a}}_n^\top \emx
        \label{eq:ch02-mat-row-rep}
    \end{equation}
\end{itemize}

\begin{boxedstuff}
    \begin{problem}
        \textbf{Elements of a row and column of a matrix $\mf{A}$} Consider a matrix $\mf{A} \in \mb{R}^{n \times m}$ with elements $a_{ij}$. Can you write down the elements of the $k^{th}$ row and the $l^{th}$ row of the matrix $\mf{A}$?.
    \end{problem}
\end{boxedstuff}
We will first start with a simplified versions of the matric multiplication problem before tackling the most general problem. It is left as an exercise for you to verify that the following four simplified version comply with Eq.~\ref{eq:ch02-mat-mult}.
\begin{itemize}
    \item \textbf{Inner product:} We have already seen this in the previous chapter. Consider two column vectors $\mf{x}, \mf{y} \in \mb{R}^n$. We can this of these two as $n \times 1$ matrices. The inner product is a product between a row matrix and a column matrix. It is defined as follows, 
    \[ \mf{x}^\top\mf{y} = \sum_{i=1}^{n} x_iy_i \]
    Note that the matrices $\mf{x}^\top \in \mb{R}^{1 \times n}$ and $\mf{y} \in \mb{R}^{n \times 1}$. Thus, matrix multiplication is defined.
    
    \item \textbf{Post-multiplying a matrix $\mf{A}$ by a column vector $\mf{x}$:} Consider a matrix $\mf{A} \in \mb{R}^{n \times m}$ and a column vector $\mf{x} \in \mb{R}^m$. The product $\mf{A}\mf{x}$ produces a column vector $\mf{y} \in \mb{R}^n$.
    \begin{equation}
        \mf{y} = \mf{A}\mf{x} = \sum_{i=1}^n x_i\mf{a}_i = x_1 \mf{a}_1 + x_2 \mf{a}_2 + \cdots + x_m\mf{a}_m
        \label{eq:ch02-mat-post-mult-col-vec}
    \end{equation}
    The above equation has a nice interpreation. The vector $\mf{y}$ is the linear combination of the columns of $\mf{A}$, where the mixture for the linear combination come from the elements of the vector $\mf{x}$. This means that post-multiplying a matrix $\mf{A}$ by a column vector is a process of generating a vector from the span of the set formed by the columns of the matrix $\mf{A}$ (recall span of a set of vector from Section~\ref{sec:ch01-span}). We will now show how Eq.~\ref{eq:ch02-mat-post-mult-col-vec} is the consequence of Eq.~\ref{eq:ch02-mat-mult}. From, Eq.~\ref{eq:ch02-mat-mult}, we have:
    \[ \begin{split}
        \mf{y} &= \mf{A}\mf{x} = \bmxc \sum_{i=1}^m a_{1i}x_i \\ \sum_{i=1}^m a_{2i}x_i \\ \vdots \\ \sum_{i=1}^m a_{ni}x_i \emx = \bmxc a_{11}x_1 + a_{12}x_2 + \cdots + a_{1m}x_m \\ a_{21}x_1 + a_{22}x_2 + \cdots + a_{2m}x_m \\ \vdots \\ a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nm}x_m \emx = \bmxc a_{11}x_1 \\ a_{21}x_1 \\ \vdots \\ a_{n1}x_1\emx + \bmxc a_{12}x_2 \\ a_{22}x_2 \\ \vdots \\ a_{n2}x_2\emx + \cdots + \bmxc a_{1m}x_m \\ a_{2m}x_m \\ \vdots \\ a_{nm}x_m \emx \\
        &= x_1 \bmxc a_{11} \\ a_{21} \\ \vdots \\ a_{n1}\emx + x_2 \bmxc a_{12} \\ a_{22} \\ \vdots \\ a_{n2}\emx + \cdots + x_m\bmxc a_{1m} \\ a_{2m} \\ \vdots \\ a_{nm} \emx = x_1 \mf{a}_1 + x_2 \mf{a}_2 + \cdots + x_m\mf{a}_m
    \end{split} \]
    
    \item \textbf{Pre-multiplying a matrix $\mf{A}$ by a row vector $\mf{x}^\top$:} Consider a matrix $\mf{A} \in \mb{R}^{n \times m}$ and a row vector $\mf{x}^\top \in \mb{R}^{1 \times n}$. The product $\mf{x}^\top\mf{A}$ produces a row vector $\mf{y} \in \mb{R}^{1 \times m}$. 
    \begin{equation}
        \mf{y} = \mf{x}^\top\mf{A} = \sum_{i=1}^m x_i\tilde{\mf{a}}_i^\top = x_1 \tilde{\mf{a}}_1^\top + x_2 \tilde{\mf{a}}_2^\top + \cdots + x_n\tilde{\mf{a}}_n^\top
        \label{eq:ch02-mat-pre-mult-row-vec}
    \end{equation}
    
    
    
    Consider a matrix $\mf{A} \in \mb{R}^{n \times m}$ and a column vector $\mf{x} \in \mb{R}^m$. The product $\mf{A}\mf{x}$ produces a column vector $\mf{y} \in \mb{R}^n$.
    \begin{equation}
        \mf{y} = \mf{A}\mf{x} = \sum_{i=1}^n x_i\mf{a}_i = x_1 \mf{a}_1 + x_2 \mf{a}_2 + \cdots + x_m\mf{a}_m
        \label{eq:ch02-mat-post-mult-col-vec}
    \end{equation}
    The above equation has a nice interpreation. The vector $\mf{y}$ is the linear combination of the columns of $\mf{A}$, where the mixture for the linear combination come from the elements of the vector $\mf{x}$. This means that post-multiplying a matrix $\mf{A}$ by a column vector is a process of generating a vector from the span of the set formed by the columns of the matrix $\mf{A}$ (recall span of a set of vector from Section~\ref{sec:ch01-span}). We will now show how Eq.~\ref{eq:ch02-mat-post-mult-col-vec} is the consequence of Eq.~\ref{eq:ch02-mat-mult}. From, Eq.~\ref{eq:ch02-mat-mult}, we have:
    \[ \begin{split}
        \mf{y} &= \mf{A}\mf{x} = \bmxc \sum_{i=1}^m a_{1i}x_i \\ \sum_{i=1}^m a_{2i}x_i \\ \vdots \\ \sum_{i=1}^m a_{ni}x_i \emx = \bmxc a_{11}x_1 + a_{12}x_2 + \cdots + a_{1m}x_m \\ a_{21}x_1 + a_{22}x_2 + \cdots + a_{2m}x_m \\ \vdots \\ a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nm}x_m \emx = \bmxc a_{11}x_1 \\ a_{21}x_1 \\ \vdots \\ a_{n1}x_1\emx + \bmxc a_{12}x_2 \\ a_{22}x_2 \\ \vdots \\ a_{n2}x_2\emx + \cdots + \bmxc a_{1m}x_m \\ a_{2m}x_m \\ \vdots \\ a_{nm}x_m \emx \\
        &= x_1 \bmxc a_{11} \\ a_{21} \\ \vdots \\ a_{n1}\emx + x_2 \bmxc a_{12} \\ a_{22} \\ \vdots \\ a_{n2}\emx + \cdots + x_m\bmxc a_{1m} \\ a_{2m} \\ \vdots \\ a_{nm} \emx = x_1 \mf{a}_1 + x_2 \mf{a}_2 + \cdots + x_m\mf{a}_m
    \end{split} \]

    \item \textbf{Outer product:}
\end{itemize}


Consider two matrices $\mf{A} \in \mb{R}^{n \times p}$ and $\mf{B} \in \mb{R}^{p \times m}$. These two matrices can be multiplied to get a matrix $\mf{C} = \mf{A}\mf{B} \in \mb{R}^{n \times m}$. Now, we are ready to look at the four views, which are described in the following subsections. 

\subsubsection{Inner product view}
The $ij^{th}$ element of $\mf{C} = \mf{A}\mf{B}$ is given by the inner product of the $i^{th}$ row of $\mf{A}$ and the $j^{th}$ column of $\mf{B}$.
\begin{equation}
    c_{ij} = \tilde{\mf{a}}_i^\top \mf{b}_j = \sum_{k=1}^{p} a_{ik}b_{kj} \implies \mf{C} = \bmxc \tilde{\mf{a}}_1^\top\mf{b}_1 & \tilde{\mf{a}}_1^\top\mf{b}_2 & \ldots & \tilde{\mf{a}}_1^\top\mf{b}_m \\
    \tilde{\mf{a}}_2^\top\mf{b}_1 & \tilde{\mf{a}}_2^\top\mf{b}_2 & \ldots & \tilde{\mf{a}}_2^\top\mf{b}_m \\
    \vdots & \vdots & \ddots & \vdots \\
    \tilde{\mf{a}}_n^\top\mf{b}_1 & \tilde{\mf{a}}_n^\top\mf{b}_2 & \ldots & \tilde{\mf{a}}_n^\top\mf{b}_m
    \emx
    \label{eq:ch02-mat-mult-inner-prod}
\end{equation}

\subsubsection{Column view}
The columns of the matrix $\mf{C} = \mf{A}\mf{B}$ are the linear combinations of the columns of the matrix $\mf{A}$, where the mixture for the linear combination come from the columns of the matrix $\mf{B}$.
\begin{equation}
    \mf{c}_i = \mf{A}\mf{b}_i
    % \mf{C} = \bmxc \mf{a}_1 & \mf{a}_2 & \ldots & \mf{a}_p \emx \bmxc \mf{b}_1 & \mf{b}_2 & \ldots & \mf{b}_m \emx = \bmxc \mf{a}_1\mf{b}_1 & \mf{a}_1\mf{b}_2 & \ldots & \mf{a}_1\mf{b}_m \\
    % \mf{a}_2\mf{b}_1 & \mf{a}_2\mf{b}_2 & \ldots & \mf{a}_2\mf{b}_m \\
    % \vdots & \vdots & \ddots & \vdots \\
    % \mf{a}_n\mf{b}_1 & \mf{a}_n\mf{b}_2 & \ldots & \mf{a}_n\mf{b}_m
    % \emx
    \label{eq:ch02-mat-mult-col-view}
\end{equation}


\subsubsection{Row view}

\subsubsection{Outer product view}